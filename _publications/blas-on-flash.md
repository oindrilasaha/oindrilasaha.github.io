---
title: "BLAS-on-flash: an alternative for training large ML models?"
collection: publications
permalink: /publication/blas-on-flash
excerpt: 'Many ML training tasks admit learning algorithms that can
be composed with linear algebra. On large datasets, the
working set of these algorithms overflows the memory. For
such scenarios, we propose a library that supports BLAS
and sparseBLAS subroutines on large matrices resident on
inexpensive non-volatile memory. We demonstrate that such
libraries can achieve near in-memory performance and be
used for fast implementations of complex algorithms such as
eigen-solvers. We believe that this approach could be a cost-
effective alternative to expensive big-data compute system'
date: 2018-02-15
venue: 'Systems for Machine Learning Conference'
paperurl: '/files/sysml18.pdf'
citation: 'Suhas Jayaram Subramanya, Srajan Garg, and Harsha Vardhan Simhadri. "BLAS-on-flash: an alternative for training large ML models?."'
---
